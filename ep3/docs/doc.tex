\documentclass[12pt]{article}

\usepackage[brazilian]{babel}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{thmtools,thm-restate}
\usepackage{amsfonts}
\usepackage{hyperref}
\usepackage[singlelinecheck=false]{caption}
\usepackage{enumitem}
\usepackage[justification=centering]{caption}
\usepackage{indentfirst}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{listings}
\usepackage[x11names,rgb,table]{xcolor}
\usepackage{tikz}
\usepackage{hyperref}
\usepackage{subcaption}
\usepackage{booktabs}
\usepackage{linegoal}
\usepackage{geometry}
\usetikzlibrary{snakes,arrows,shapes}

\graphicspath{{imgs/}}

\makeatletter
\def\subsection{\@startsection{subsection}{3}%
  \z@{.5\linespacing\@plus.7\linespacing}{.1\linespacing}%
  {\normalfont}}
\makeatother

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\Val}{\text{Val}}
\DeclareMathOperator*{\Ch}{\text{Ch}}
\DeclareMathOperator*{\Pa}{\text{Pa}}
\DeclareMathOperator*{\Sc}{\text{Sc}}
\newcommand{\ov}{\overline}
\newcommand{\tsup}{\textsuperscript}

\newcommand\defeq{\mathrel{\overset{\makebox[0pt]{\mbox{\normalfont\tiny\sffamily def}}}{=}}}

\newcommand{\algorithmautorefname}{Algorithm}
\algrenewcommand\algorithmicrequire{\textbf{Input}}
\algrenewcommand\algorithmicensure{\textbf{Output}}
\algnewcommand{\LineComment}[1]{\State\,\(\triangleright\) #1}

\captionsetup[table]{labelsep=space}

\theoremstyle{plain}

\newcounter{dummy-def}\numberwithin{dummy-def}{section}
\newtheorem{definition}[dummy-def]{Definition}
\newcounter{dummy-thm}\numberwithin{dummy-thm}{section}
\newtheorem{theorem}[dummy-thm]{Theorem}
\newcounter{dummy-prop}\numberwithin{dummy-prop}{section}
\newtheorem{proposition}[dummy-prop]{Proposition}
\newcounter{dummy-corollary}\numberwithin{dummy-corollary}{section}
\newtheorem{corollary}[dummy-corollary]{Corollary}
\newcounter{dummy-lemma}\numberwithin{dummy-lemma}{section}
\newtheorem{lemma}[dummy-lemma]{Lemma}
\newcounter{dummy-ex}\numberwithin{dummy-ex}{section}
\newtheorem{exercise}[dummy-ex]{Exercise}
\newcounter{dummy-eg}\numberwithin{dummy-eg}{section}
\newtheorem{example}[dummy-eg]{Example}

\numberwithin{equation}{section}

\newcommand{\set}[1]{\mathbf{#1}}
\newcommand{\pr}{\mathbb{P}}
\newcommand{\eps}{\varepsilon}
\newcommand{\ddspn}[2]{\frac{\partial#1}{\partial#2}}
\newcommand{\iddspn}[2]{\partial#1/\partial#2}
\renewcommand{\implies}{\Rightarrow}

\newcommand{\bigo}{\mathcal{O}}

\setlength{\parskip}{1em}

\lstset{frameround=fttt,
	numbers=left,
	breaklines=true,
	keywordstyle=\bfseries,
	basicstyle=\ttfamily,
}

\newcommand{\code}[1]{\lstinline[mathescape=true]{#1}}
\newcommand{\mcode}[1]{\lstinline[mathescape]!#1!}

\newgeometry{margin=1in}
\title{%
  Relatório MAC0318 EP3\\
  \author{Renato Lui Geh --- NUSP\@: 8536030}
}
\date{}

\begin{document}

\maketitle

\section{Classificador}

Para a classificação, foi usada uma ``amostra'' das imagens. Para treinar pelas regras, usou-se as
médias das intensidades dos pixels de certas regiões selecionadas. Chame de $R$ uma região da
imagem. Foi atribuída uma média $\mu_R(D)$ onde $D$ é a amostra das imagens, separando as imagens
pelos seus respectivos rótulos de classificação.

Para classificar uma imagem $I$, computou-se as médias das mesmas regiões que na amostragem.
Seja $\mu_{R_1},\mu_{R_2},\ldots,\mu_{R_n}$ as médias computadas nas amostragens e
$\hat{\mu_{R_1}},\hat{\mu_{R_2}},\ldots,\hat{\mu_{R_n}}$ as médias computadas na imagem $I$ para
classificação. O erro acumulado $E_I$ é dado por:

\begin{equation*}
  \sum_{i=1}^n |\mu_{R_i}-\hat{\mu_{R_i}}|
\end{equation*}

Por fim, queremos achar:

\begin{equation*}
  \argmax_{l\in L} E_I = \argmax_{l\in L} \sum_{i=1}^n |\mu_{R_i}-\hat{\mu_{R_i}}|
\end{equation*}

Onde $L$ é o conjunto de classes.

\section{Estratégias}

Para achar as regiões $R$ mencionadas na seção anterior, foram escolhidas as seguintes regiões da
imagem:

\begin{enumerate}
  \item Matriz triangular L na $i$-ésima diagonal;
  \item Matriz triangular U na $i$-ésima diagonal;
  \item ``Reflexo'' da matriz triangular L na $i$-ésima diagonal;
  \item ``Reflexo'' da matriz triangular U na $i$-ésima diagonal;
  \item Regiões retangulares de dimensão $(s_x, s_y)$;
  \item Partições retangulares principais;
  \item Quadrantes;
  \item ``Teto'' e ``chão''.
\end{enumerate}

Para as matrizes triangulares, usou-se \code{np.tril} e \code{np.triu}. Sejam $L_i$ e $U_i$ as matrizes
triangulares L e U respectivamente na diagonal $i$. O ``reflexo'' da matriz triangular $L_i$ e
$U_i$ são dadas por $M-L_i$ e $M-U_i$ respectivamente, onde $M$ é a matriz da imagem original.

As regiões retangulares são todas as regiões de tamanho $(s_x, s_y)$ nas posições $(i\cdot s_x,
j\cdot s_y)$, com $i\in\mathbb{Z}_w, j\in\mathbb{Z}_h$, onde $(w, h)$ são as dimensões da imagem.

As partições retangulares principas são as regiões:

\begin{enumerate}
  \item $(0, 0, w/2, h/2)$
  \item $(0, 0, w/2, h)$
  \item $(0, h/2, w/2, h)$
  \item $(w/2, 0, w, h/2)$
  \item $(w/2, 0, w, h)$
  \item $(w/2, h/2, w, h)$
\end{enumerate}

Os quadrantes são dados pelas regiões abaixo, com $i\in\{2, 3, 4, 5\}$:

\begin{enumerate}
  \item $(0, 0, w/i, h/i)$
  \item $(0, h-h/i, w/i, h)$
  \item $(w-w/i, h, w, h-h/i)$
  \item $(w-w/i, 0, w, h/i)$
\end{enumerate}

Para o ``teto'' e ``chão'', escolheram-se as regiões ``mais pra baixo'' e ``mais pra cima'' da
imagem. Seja $3\leq i\leq 10$, com $i\in\mathbb{N}$.

\begin{enumerate}
  \item $(0, h-h/i, w, h)$
  \item $(0, 0, w, h/i)$
  \item $(0, h/i, w, h-h/i)$
  \item $(w-w/i, 0, w/i, h)$
  \item $(0, 0, w/i, h)$
  \item $(w/i, h/i, 0, h)$
  \item $(w/i, h/i, w-w/i, h-h/i)$
\end{enumerate}

\section{Resultados}

A acurácia não foi muito boa. Ficou em $\approx 68$\%.

\section{Outras estratégias}

Pensei também em duas outras coisas que poderiam melhorar: separar as médias em quantis e dar pesos
às regiões.

Para a separação em quantis, a ideia era de, ao invés de tomar a média de todas as intensidades,
separar as intensidades em quantis, para que valores extremos fossem tomados em consideração, e não
fossem absorvidos pela média. Para isso, ao invés de computar a média diretamente, as médias foram
separadas em clusters. Rodou-se k-means com $q$ clusters, e para cada cluster $q_i$, computou-se a
média. Na classificação, tomava-se em consideração o erro:

\begin{equation*}
  E_I = \sum_{j=1}^q |q_j|\left( \sum_{i=1}^n |\mu_{R_i}(q_j)-\hat{\mu_{R_i}}(q_j)|\right)
\end{equation*}

Onde $\mu_{R_i}(q_j)$ é a média da região $R_i$ apenas no cluster $q_j$. Nos resultados, usando
esta estratégias, aumentou-se em apenas 1 a 2\% de acurácia, mas demorou muito mais para treinar e
classificar.

A outra estratégia era dar pesos às regiões. A partir de um vetor $W=(w_1,\ldots,w_n)$, foi criada
uma matriz diagonal $\mathbf{W}$ em que $\mathbf{w}_{ii}=w_i$ e todas as outras entradas são zero.
O erro então é dado por:

\begin{equation*}
  E_I = \sum {(M-\hat{M})}^{\text{T}}\cdot\mathbf{W}
\end{equation*}

Onde $M$ e $\hat{M}$ são as médias da amostra e da imagem $I$ em forma matricial respectivamente. No
final, achar bons valores para $W$ foi o problema.  Além disso, a multiplicação de matrizes acima
demorava muito tempo no Raspberry Pi.

\end{document}
